{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS Observability Accelerator for CDK","text":"<p>Welcome to the AWS Observability Accelerator for CDK!</p> <p>The AWS Observability Accelerator for CDK is a set of opinionated modules to help you set up observability for your AWS environments with AWS Native services and AWS-managed observability services such as Amazon Managed Service for Prometheus,Amazon Managed Grafana, AWS Distro for OpenTelemetry (ADOT) and Amazon CloudWatch.</p> <p>We provide curated metrics, logs, traces collection, cloudwatch dashboard, alerting rules and Grafana dashboards for your EKS infrastructure, Java/JMX, NGINX based workloads and your custom applications.</p>"},{"location":"#single-eks-cluster-aws-native-observability-accelerator","title":"Single EKS Cluster AWS Native Observability Accelerator","text":""},{"location":"#single-eks-cluster-open-source-observability-accelerator","title":"Single EKS Cluster Open Source Observability Accelerator","text":""},{"location":"#patterns","title":"Patterns","text":"<p>The individual patterns can be found in the <code>lib</code> directory.  Most of the patterns are self-explanatory, for some more complex examples please use this guide and docs/patterns directory for more information.</p>"},{"location":"#usage","title":"Usage","text":"<p>Before proceeding, make sure AWS CLI is installed on your machine.</p> <p>To use the eks-blueprints and patterns module, you must have Node.js and npm installed. You will also use <code>make</code> and <code>brew</code> to simplify build and other common actions.</p>"},{"location":"#ubuntu-setup","title":"Ubuntu Setup","text":"<p>Follow the below steps to setup and leverage cdk-aws-observability-accelerator in your Ubuntu Linux machine.</p> <ol> <li>Update the package list</li> </ol> <p>Update the package list to ensure you're installing the latest versions.</p> <pre><code>sudo apt update\n</code></pre> <ol> <li>Install make</li> </ol> <pre><code>sudo apt install make\n</code></pre> <ol> <li>Install Node.js and npm</li> </ol> <p>Install Node.js and npm using the NodeSource binary distributions.</p> <pre><code>curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - &amp;&amp;\\\nsudo apt-get install -y nodejs\n</code></pre> <p>Note: The Node.js package from NodeSource includes npm</p> <ol> <li>Verify Node.js and npm Installation</li> </ol> <p>Check the installed version of Node.js:</p> <pre><code>node -v\n</code></pre> <p>The output should be <code>v20.x.x</code>.</p> <p>Check the installed version of npm:</p> <pre><code>npm -v\n</code></pre> <p>The output should be a version greater than <code>9.7.x</code>.</p> <p>If your npm version is not <code>9.7.x</code> or above, update npm with the following command:</p> <pre><code>sudo npm install -g npm@latest\n</code></pre> <p>Verify the installed version by running <code>npm -v</code>.</p> <ol> <li>Install brew on ubuntu by following instructions as detailed in docs.brew.sh</li> </ol> <pre><code> /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Add Homebrew to your PATH</p> <pre><code>test -d ~/.linuxbrew &amp;&amp; eval \"$(~/.linuxbrew/bin/brew shellenv)\"\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval \"$(/home/linuxbrew/.linux  brew/bin/brew shellenv)\"\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.bash_profile\necho \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.profile\n</code></pre> <p>Post completing the above, continue from Step: Repo setup</p>"},{"location":"#mac-setup","title":"Mac Setup:","text":"<p>Follow the below steps to setup and leverage <code>cdk-aws-observability-accelerator</code> in your local Mac laptop.</p> <ol> <li>Install <code>make</code> and <code>node</code> using brew</li> </ol> <pre><code>brew install make\nbrew install node\n</code></pre> <ol> <li>Install <code>npm</code></li> </ol> <pre><code>sudo npm install -g n\nsudo n stable\n</code></pre> <ol> <li> <p>Make sure the following pre-requisites are met:</p> </li> <li> <p>Node version is a current stable node version 18.x.</p> </li> </ol> <pre><code>$ node -v\nv20.3.1\n</code></pre> <p>Update (provided Node version manager is installed): <code>n stable</code>. May require <code>sudo</code>.</p> <ul> <li>NPM version must be 8.4 or above:</li> </ul> <pre><code>$ npm -v\n9.7.2\n</code></pre> <p>Updating npm: <code>sudo n stable</code> where stable can also be a specific version above 8.4. May require <code>sudo</code>.</p>"},{"location":"#repo-setup","title":"Repo setup","text":"<ol> <li>Clone the <code>cdk-aws-observability-accelerator</code> repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <p>PS: If you are contributing to this repo, please make sure to fork the repo, add your changes and create a PR against it.</p> <ol> <li> <p>Once you have cloned the repo, you can open it using your favourite IDE and run the below commands to install the dependencies and build the existing patterns.</p> </li> <li> <p>Install project dependencies.</p> </li> </ol> <pre><code>make deps\n</code></pre> <ul> <li>To view patterns that are available to be deployed, execute the following:</li> </ul> <pre><code>make build\n</code></pre> <ul> <li>To list the existing CDK AWS Observability Accelerator Patterns</li> </ul> <pre><code>make list\n</code></pre> <p>Note: Some patterns have a hard dependency on AWS Secrets (for example GitHub access tokens). Initially you will see errors complaining about lack of the required secrets. It is normal. At the bottom, it will show the list of patterns which can be deployed, in case the pattern you are looking for is not available, it is due to the hard dependency which can be fixed by following the docs specific to those patterns.</p> <pre><code>To work with patterns use:\n    $ make pattern &lt;pattern-name&gt; &lt;list | deploy | synth | destroy&gt;\nExample:\n    $ make pattern single-new-eks-opensource-observability deploy\n\nPatterns:\n\n    existing-eks-awsnative-observability\n    existing-eks-mixed-observability\n    existing-eks-opensource-observability\n    multi-acc-new-eks-mixed-observability\n    single-new-eks-awsnative-fargate-observability\n    single-new-eks-awsnative-observability\n    single-new-eks-cluster\n    single-new-eks-gpu-opensource-observability\n    single-new-eks-graviton-opensource-observability\n    single-new-eks-mixed-observability\n    single-new-eks-opensource-observability\n</code></pre> <ul> <li>Bootstrap your CDK environment.</li> </ul> <pre><code>npx cdk bootstrap\n</code></pre> <ul> <li>You can then deploy a specific pattern with the following:</li> </ul> <pre><code>make pattern single-new-eks-opensource-observability deploy\n</code></pre> <ul> <li>To access instructions for individual patterns check documentation in <code>docs/patterns</code> directory.</li> </ul>"},{"location":"#developer-flow","title":"Developer Flow","text":""},{"location":"#modifications","title":"Modifications","text":"<p>All files are compiled to the dist folder including <code>lib</code> and <code>bin</code> directories. For iterative development (e.g. if you make a change to any of the patterns) make sure to run compile:</p> <pre><code>make compile\n</code></pre> <p>The <code>compile</code> command is optimized to build only modified files and is fast.</p>"},{"location":"#new-patterns","title":"New Patterns","text":"<p>To create a new pattern, please follow these steps:</p> <ol> <li>Under lib create a folder for your pattern, such as <code>&lt;pattern-name&gt;-pattern</code>. If you plan to create a set of patterns that represent a particular subdomain, e.g. <code>security</code> or <code>hardening</code>, please create an issue to discuss it first. If approved, you will be able to create a folder with your subdomain name and group your pattern constructs under it.</li> <li>Blueprints generally don't require a specific class, however we use a convention of wrapping each pattern in a plain class like <code>&lt;Pattern-Name&gt;Pattern</code>. This class is generally placed in <code>index.ts</code> under your pattern folder.</li> <li>Once the pattern implementation is ready, you need to include it in the list of the patterns by creating a file <code>bin/&lt;pattern-name&gt;.ts</code>. The implementation of this file is very light, and it is done to allow patterns to run independently.</li> </ol> <p>Example simple synchronous pattern:</p> <pre><code>import SingleNewEksOpenSourceobservabilityPattern from '../lib/single-new-eks-opensource-observability-pattern';\nimport { configureApp } from '../lib/common/construct-utils';\n\nconst app = configureApp();\n\nnew SingleNewEksOpenSourceobservabilityPattern(app, 'single-new-eks-opensource');\n // configureApp() will create app and configure loggers and perform other prep steps\n</code></pre>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by the Solutions Architects from the AWS observability team with support from the AWS service teams and other volunteers from across the organization.</p> <p>Our goal is to make it easier to use AWS Native and Open Source Observability Services.</p> <p>The core team include the following people:</p> <ul> <li>Elamaran Shanmugam</li> <li>Imaya Kumar Jagannathan</li> <li>Kevin Lewin</li> <li>Michael Hausenblas</li> <li>Mikhail Shapirov</li> <li>Rodrigue Koffi</li> </ul> <p>We welcome the wider open source community and thank those who contribute to this project.</p> <p>Note that all information published on this site is available via the Apache 2.0 license.</p>"},{"location":"logs/","title":"Viewing Logs","text":"<p>By default, we deploy a FluentBit daemon set in the cluster to collect worker logs for all namespaces. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.</p> <p>Further configuration options are available in the module documentation. This guide shows how you can leverage either CloudWatch Logs or Amazon Managed Grafana for your cluster and application logs.</p>"},{"location":"logs/#viewing-logs-in-cloudwatch-logs-insights","title":"Viewing Logs in CloudWatch Logs Insights","text":"<p>Navigate to CloudWatch, then go to \"Logs Insights\"</p> <p>In the dropdown, select any of the logs that begin with \"/aws/eks/single-new-eks-mixed-observability-accelerator\" and run a query.</p> <p>Example with \"kubesystem\" log group:</p> <p></p> <p>Then you can view the results of your query:</p> <p></p>"},{"location":"logs/#viewing-logs-in-grafana","title":"Viewing Logs in Grafana","text":""},{"location":"logs/#using-cloudwatch-logs-as-data-source-in-grafana","title":"Using CloudWatch Logs as data source in Grafana","text":"<p>Follow the documentation to enable Amazon CloudWatch as a data source. Make sure to provide permissions.</p> <p>All logs are delivered in the following CloudWatch Log groups naming pattern: <code>/aws/eks/$PATTERN</code>. Log streams follow <code>{container-name}.{pod-name}</code>. In Grafana, querying and analyzing logs is done with CloudWatch Logs Insights</p>"},{"location":"logs/#example-adot-collector-logs","title":"Example - ADOT collector logs","text":"<p>Select one or many log groups and run the following query. The example below, queries AWS Distro for OpenTelemetry (ADOT) logs</p> <pre><code>fields @timestamp, log\n| order @timestamp desc\n| limit 100\n</code></pre> <p></p>"},{"location":"logs/#example-using-time-series-visualizations","title":"Example - Using time series visualizations","text":"<p>CloudWatch Logs syntax provide powerful functions to extract data from your logs. The <code>stats()</code> function allows you to calculate aggregate statistics with log field values. This is useful to have visualization on non-metric data from your applications.</p> <p>In the example below, we use the following query to graph the number of metrics collected by the ADOT collector</p> <pre><code>fields @timestamp, log\n| parse log /\"#metrics\": (?&lt;metrics_count&gt;\\d+)}/\n| stats avg(metrics_count) by bin(5m)\n| limit 100\n</code></pre> <p>Tip</p> <p>You can add logs in your dashboards with logs panel types or time series depending on your query results type.</p> <p></p> <p>Warning</p> <p>Querying CloudWatch logs will incur costs per GB scanned. Use small time windows and limits in your queries. Checkout the CloudWatch pricing page for more info.</p>"},{"location":"support/","title":"Support &amp; Feedback","text":"<p>AWS Observability Accelerator for CDK is maintained by AWS Solution Architects. It is not part of an AWS service and support is provided best-effort by the AWS Observability Accelerator community.</p> <p>To post feedback, submit feature ideas, or report bugs, please use the issues section of this GitHub repo.</p> <p>If you are interested in contributing, see the contribution guide.</p>"},{"location":"tracing/","title":"Tracing on Amazon EKS","text":"<p>Distributed tracing helps you have end-to-end visibility between transactions in distributed nodes. The <code>eks-monitoring</code> module is configured  by default to collect traces into AWS X-Ray.</p> <p>The AWS Distro for OpenTelemetry collector is configured to receive traces in the OTLP format (OTLP receiver), using the OpenTelemetry SDK or auto-instrumentation agents.</p> <p>Note</p> <p>To disable the tracing configuration, <code>XrayAdotAddOn</code> for Mixed and Open Source Observability Accelerators from the CDK Observability Patterns</p>"},{"location":"tracing/#instrumentation","title":"Instrumentation","text":"<p>Let's take a sample application that is already instrumented with the OpenTelemetry SDK.</p> <p>Note</p> <p>To learn more about instrumenting with OpenTelemetry, please visit the OpenTelemetry documentation for your programming language.</p> <p>Cloning the repo</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git\ncd aws-otel-community/sample-apps/go-sample-app\n</code></pre> <p>Highlighting code sections</p>"},{"location":"tracing/#deploying-on-amazon-eks","title":"Deploying on Amazon EKS","text":"<p>Using the sample application, we will build a container image, create and push an image on Amazon ECR. We will use a Kubernetes manifest to deploy to an EKS cluster.</p> <p>Warning</p> <p>The following steps require that you have an EKS cluster ready. To deploy an EKS cluster, please visit our example.</p>"},{"location":"tracing/#building-container-image","title":"Building container image","text":"amd64 linuxcross platform build <pre><code>docker build -t go-sample-app .\n</code></pre> <pre><code>docker buildx build -t go-sample-app . --platform=linux/amd64\n</code></pre>"},{"location":"tracing/#publishing-on-amazon-ecr","title":"Publishing on Amazon ECR","text":"using docker <pre><code>export ECR_REPOSITORY_URI=$(aws ecr create-repository --repository go-sample-app --query repository.repositoryUri --output text)\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REPOSITORY_URI\ndocker tag go-sample-app:latest \"${ECR_REPOSITORY_URI}:latest\"\ndocker push \"${ECR_REPOSITORY_URI}:latest\"\n</code></pre>"},{"location":"tracing/#deploying-on-amazon-eks_1","title":"Deploying on Amazon EKS","text":"eks.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: go-sample-app\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: go-sample-app\n  template:\n    metadata:\n      labels:\n        app: go-sample-app\n    spec:\n      containers:\n        - name: go-sample-app\n          image: \"${ECR_REPOSITORY_URI}:latest\" # make sure to replace this variable\n          imagePullPolicy: Always\n          env:\n          - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\n            value: adot-collector.adot-collector-kubeprometheus.svc.cluster.local:4317\n          resources:\n            limits:\n              cpu:  300m\n              memory: 300Mi\n            requests:\n              cpu: 100m\n              memory: 180Mi\n          ports:\n            - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: go-sample-app\n  namespace: default\n  labels:\n    app: go-sample-app\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: go-sample-app\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: go-sample-app\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: go-sample-app\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n</code></pre>"},{"location":"tracing/#deploying-and-testing","title":"Deploying and testing","text":"<p>With the Kubernetes manifest ready, run:</p> <pre><code>kubectl apply -f eks.yaml\n</code></pre> <p>You should see the pods running with the command:</p> <pre><code>kubectl get pods\nNAME                              READY   STATUS    RESTARTS        AGE\ngo-sample-app-67c48ff8c6-bdw74    1/1     Running   0               4s\ngo-sample-app-67c48ff8c6-t6k2j    1/1     Running   0               4s\n</code></pre> <p>To simulate some traffic you can forward the service port to your local host and test a few queries</p> <pre><code>kubectl port-forward deployment/go-sample-app 8080:8080\n</code></pre> <p>Test a few endpoints</p> <pre><code>curl http://localhost:8080/\ncurl http://localhost:8080/outgoing-http-call\ncurl http://localhost:8080/aws-sdk-call\ncurl http://localhost:8080/outgoing-sampleapp\n</code></pre>"},{"location":"tracing/#visualizing-traces","title":"Visualizing traces","text":"<p>As this is a basic example, the service map doesn't have a lot of nodes, but this shows you how to setup tracing in your application and deploying it on Amazon EKS using our OSS observability patterns.</p> <p>With Flux and Grafana Operator, the OSS pattern configures an AWS X-Ray data source on your provided Grafana workspace. Open the Grafana explorer view and select the X-Ray data source. If you type the query below, and select <code>Trace List</code> for Query Type, you should see the list of traces occured in the selected timeframe.</p> <p></p> <p>You can add the service map to a dashbaord, for example a service focused dashbaord. You can click on any of the traces to view a node map and the traces details.</p> <p>There is a button that can take you the CloudWatch console to view the same data. If your logs are stored on CloudWatch Logs, this page can present all the logs in the trace details page. The CloudWatch Log Group name should be added to the trace as an attribute. Read more about this in our One Observability Workshop</p> <p></p>"},{"location":"tracing/#resoures","title":"Resoures","text":"<ul> <li>AWS Observability Best Practices</li> <li>One Observability Workshop</li> <li>AWS Distro for OpenTelemetry documentation</li> <li>AWS X-Ray user guide</li> <li>OpenTelemetry documentation</li> </ul>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/","title":"Single Cluster Open Source Observability - API Server Monitoring","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and adds API server monitoring, with open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#deploying","title":"Deploying","text":"<ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys.</li> </ol> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_WORKLOADS_API_BASIC_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_WORKLOADS_API_ADVANCED_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_WORKLOADS_API_TROUBLESHOOTING_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"apiserver.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>Kubernetes/Kube-apiserver (basic), Kubernetes/Kube-apiserver (advanced), Kubernetes/Kube-apiserver (troubleshooting)</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (basic)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (advanced)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (troubleshooting)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New Nginx Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/","title":"Single Cluster AWS Native Observability","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster AWS Native Observability pattern, using AWS native tools such as CloudWatch and Logs and Open Source tools such as AWS Distro for OpenTelemetry (ADOT).</p> <p>[!NOTE] Currently, Xray AddOn is not supported for imported clusters. The Xray AddOn requires access to the nodegroup which is not available with the imported cluster and it does not support IRSA as of now and requires modification of the node instance role. Once it supports IRSA, we would update this pattern to work with the existing clusters.</p> <p></p> <p>This example makes use of CloudWatch, as a metric and log aggregation layer. In order to collect the metrics and traces, we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster, with AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#deploying","title":"Deploying","text":"<ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-awsnative-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. <pre><code>aws eks update-kubeconfig --name single-new-eks-observability-accelerator --region &lt;your-region&gt; --role-arn arn:aws:iam::**************:role/single-new-eks-observabil-singleneweksobservabilit-CPAN247ASDF\n</code></pre> Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output: <pre><code>NAME                           STATUS   ROLES    AGE   VERSION\nip-10-0-145-216.ec2.internal   Ready    &lt;none&gt;   14m   v1.25.11-eks-a5565ad\n</code></pre></p> <p>Next, lets verify the namespaces in the cluster: <pre><code>kubectl get ns # Output shows all namespace\n</code></pre></p> <p>Output: <pre><code>NAME                       STATUS   AGE\namazon-metrics             Active   4m31s\naws-for-fluent-bit         Active   4m31s\ncert-manager               Active   4m31s\ndefault                    Active   24m\nkube-node-lease            Active   24m\nkube-public                Active   24m\nkube-system                Active   24m\nprometheus-node-exporter   Active   13m\n</code></pre></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#visualization","title":"Visualization","text":"<ul> <li>Navigate to CloudWatch &gt; Insights &gt; Container Insights and select cluster, select <code>single-new-eks-cluster</code> if you created cluster with pattern mentioned from above guide, otherwise select relevant cluster.</li> <li>Now select <code>amazon-metrics</code> namepsace </li> <li> <p>On a same view, select 'EKS Pods', which provides insights overview of all the pods as shown below </p> </li> <li> <p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p> </li> </ul>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-awsnative-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/","title":"Single Cluster AWS Mixed Observability","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster AWS Mixed Observability pattern, using AWS native tools such as CloudWatch and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry (ADOT) and Prometheus Node Exporter.</p> <p></p> <p>This example makes use of CloudWatch, as a metric and log aggregation layer, while X-Ray is used as a trace-aggregation layer. In order to collect the metrics and traces, we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p> <p>In this architecture, AWS X-Ray provides a complete view of requests as they travel through your application and filters visual data across payloads, functions, traces, services, and APIs. X-Ray also allows you to perform analytics, to gain powerful insights about your distributed trace data.</p> <p>Utilizing CloudWatch and X-Ray as an aggregation layer allows for a fully-managed scalable telemetry backend. In this example we get those benefits while still having the flexibility and rapid development of the Open Source collection tools.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster, with a mixture of AWS native and open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#deploying","title":"Deploying","text":"<ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-mixed-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New EKS Cluster AWS Mixed Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-mixed-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/","title":"Single Cluster Open Source Observability - NGINX Monitoring","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster NGINX pattern, using Open Source tools such as AWS Distro for OpenTelemetry (ADOT), Amazon Managed Grafana workspace and Prometheus.</p> <p>The current example deploys the AWS Distro for OpenTelemetry Operator for Amazon EKS with its requirements and make use of an existing Amazon Managed Grafana workspace. It creates a new Amazon Managed Service for Prometheus workspace. And You will gain both visibility on the cluster and NGINX based applications.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and NGINX workloads, with open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#deploying","title":"Deploying","text":"<ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys.</li> </ol> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NGINX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/nginx\"\n        }\n      ]\n    },\n    \"nginx.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#deploy-an-example-nginx-application","title":"Deploy an example Nginx application","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector.</p> <ol> <li> <p>Add NGINX ingress controller add-on into lib/existing-eks-opensource-observability-pattern/index.ts in add-on array. <pre><code>        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.XrayAdotAddOn(),\n            new blueprints.addons.FluxCDAddOn({\"repositories\": [fluxRepository]}),\n            new GrafanaOperatorSecretAddon(),\n            new blueprints.addons.NginxAddOn({\n                name: \"ingress-nginx\",\n                chart: \"ingress-nginx\",\n                repository: \"https://kubernetes.github.io/ingress-nginx\",\n                version: \"4.7.2\",\n                namespace: \"nginx-ingress-sample\",\n                values: {\n                    controller: { \n                        metrics: {\n                            enabled: true,\n                            service: {\n                                annotations: {\n                                    \"prometheus.io/port\": \"10254\",\n                                    \"prometheus.io/scrape\": \"true\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }),\n        ];\n</code></pre></p> </li> <li> <p>Deploy pattern again  <pre><code>make pattern existing-eks-opensource-observability deploy\n</code></pre></p> </li> <li> <p>Verify if the application is running <pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre></p> </li> <li> <p>Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller. <pre><code>EXTERNAL_IP=your-nginx-controller-external-ip\n</code></pre></p> </li> <li> <p>Start some sample NGINX traffic by entering the following command. <pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/master/k8s-deployment-manifest-templates/deployment-mode/service/cwagent-prometheus/sample_traffic/nginx-traffic/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f -\n</code></pre></p> </li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pod -n nginx-sample-traffic \n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#visualization","title":"Visualization","text":"<ol> <li>Prometheus datasource on Grafana</li> <li> <p>After a successful deployment, this will open the Prometheus datasource configuration on Grafana. You should see a notification confirming that the Amazon Managed Service for Prometheus workspace is ready to be used on Grafana.</p> </li> <li> <p>Grafana dashboards</p> </li> <li>Go to the Dashboards panel of your Grafana workspace. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code>.</li> </ol> <p> </p> <ol> <li>Amazon Managed Service for Prometheus rules and alerts</li> <li>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the Rules management tab, you should find new rules deployed.</li> </ol> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#verify-the-resources_1","title":"Verify the resources","text":"<p>Please see Single New Nginx Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/","title":"Single Cluster Open Source Observability","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster Open Source Observability pattern, using AWS native tools such as CloudWatch, Amazon Managed Service for Prometheus, Amazon Managed Grafana, and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry (ADOT) and Prometheus Node Exporter.</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#objective","title":"Objective","text":"<p>Configure the existing Amazon EKS cluster with below Observability components; - AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces - Logs with AWS for FluentBit - Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana. - Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this. - Installs External Secrets Operator to retrieve and Sync the Grafana API keys. - Amazon Managed Grafana Dashboard and data source - Alerts and recording rules with Amazon Managed Service for Prometheus</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys.</li> </ol> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <p>If you need Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>If you want to deploy API Server dashboards along with Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true,\n    \"apiserver.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New EKS Open Source Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/","title":"Multi-Cluster Multi-Account Multi-Region (M3) Observability","text":""},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Multi-Account Multi-Region Mixed Observability (M3) Accelerator using both AWS native tooling such as: CloudWatch ContainerInsights, CloudWatch logs and Open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana :</p> <p></p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#objective","title":"Objective","text":"<ol> <li>Deploying two production grade Amazon EKS cluster across two AWS Accounts (Prod1, Prod2 account) in two different regions through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in another AWS account (pipeline account).</li> <li>Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote-write metrics to AMP workspace in Prod 1 AWS Account.</li> <li>Deploying ADOT add-on, CloudWatch add-on to Prod 2 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account.</li> <li>Configuring GitOps tooling (Argo CD add-on) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using Argo CD projects.</li> <li>Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (mon-account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account.</li> <li>Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2.</li> <li>Installing Grafana Operator in Monitoring account (mon-account) to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installing External Secrets Operator in Monitoring account (mon-account) to retrieve and Sync the Grafana API keys.</li> </ol>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#gitops-configuration","title":"GitOps configuration","text":"<ul> <li>For GitOps, this pattern bootstraps Argo CD add-on and points to sample applications in AWS Observability Accelerator.</li> <li>You can find the team-geordie configuration for this pattern in the workload repository under the folder <code>team-geordie</code>.</li> <li>GitOps based management of Amazon Grafana resources (like: Datasources and Dashboards) is achieved using Argo CD application <code>grafana-operator-app</code>. Grafana Operator resources are deployed using <code>grafana-operator-chart</code>.</li> </ul>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure following tools are installed in host machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>argocd</li> <li>jq</li> </ol>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#aws-accounts","title":"AWS Accounts","text":"<ol> <li>AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation, or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO.</li> <li>An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka <code>prodEnv1</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka <code>prodEnv2</code>) provisioned using the AWS Service Catalog Account Factory] product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Pipeline Account (aka <code>pipelineEnv</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka <code>monitoringEnv</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An existing Amazon Managed Grafana Workspace in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. Enable Data sources AWS X-Ray, Amazon Managed Service for Prometheus and Amazon Cloudwatch.</li> <li>If you are bringing new AWS accounts to deploy this pattern, then create a free-tier EC2 instance and let it run for 15-30 minutes in order to complete validation of account.</li> </ol> <p>NOTE: This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created in <code>ProdEnv1</code>, <code>ProdEnv2</code> and <code>monitoringEnv</code> AWS accounts. Make sure your account limits for EIP are increased to support additional 3 EIPs per account.</p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#clone-repository","title":"Clone Repository","text":"<p>Clone <code>cdk-aws-observability-accelerator</code> repository, if not done already.</p> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\ncd cdk-aws-observability-accelerator\n</code></pre> <p>Pro Tip: This document is compatible to run as Notebook with RUNME for VS Code. There's no need to manually copy and paste commands. You can effortlessly execute them directly from this markdown file. Feel free to give it a try.</p> <p>Here is a sample usage of this document using RUNME:</p> <p></p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#sso-profile-setup","title":"SSO Profile Setup","text":"<ol> <li>You will be accessing multiple accounts during deployment of this pattern. It is recommended to configure the AWS CLI to authenticate access with AWS IAM Identity Center (successor to AWS Single Sign-On). Let's configure Token provider with automatic authentication refresh for AWS IAM Identity Center. Ensure Prerequisites mentioned here are complete before proceeding to next steps.</li> <li>Create and use AWS IAM Identity Center login with <code>AWSAdministratorAccess</code> Permission set assigned to all AWS accounts required for this pattern (prodEnv1, prodEnv2, pipelineEnv and monitoringEnv).</li> <li>Configure AWS profile with sso for <code>pipelineEnv</code> account:</li> </ol> <pre><code>aws configure sso --profile pipeline-account\n</code></pre> <pre><code># sample configuration\n# SSO session name (Recommended): coa-multi-access-sso\n# SSO start URL [None]: https://d-XXXXXXXXXX.awsapps.com/start\n# SSO region [None]: us-west-2\n# SSO registration scopes [sso:account:access]:sso:account:access\n\n# Attempting to automatically open the SSO authorization page in your default browser.\n# If the browser does not open or you wish to use a different device to authorize this request, open the following URL:\n\n# https://device.sso.us-west-2.amazonaws.com/\n\n# Then enter the code:\n\n# XXXX-XXXX\n# There are 7 AWS accounts available to you.\n# Using the account ID 111122223333\n# There are 2 roles available to you.\n# Using the role name \"AWSAdministratorAccess\"\n# CLI default client Region [None]: us-west-2\n# CLI default output format [None]: json\n\n# To use this profile, specify the profile name using --profile, as shown:\n\n# aws s3 ls --profile pipeline-account\n</code></pre> <ol> <li>Then, configure profile for <code>ProdEnv1</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile prod1-account\n</code></pre> <pre><code># sample configuration\n# SSO session name (Recommended): coa-multi-access-sso\n# There are 7 AWS accounts available to you.\n# Using the account ID 444455556666\n# There are 2 roles available to you.\n# Using the role name \"AWSAdministratorAccess\"\n# CLI default client Region [None]: us-west-2\n# CLI default output format [None]: json\n\n# To use this profile, specify the profile name using --profile, as shown:\n\n# aws s3 ls --profile prod2-account\n</code></pre> <ol> <li>Then, configure profile for <code>ProdEnv2</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile prod2-account\n</code></pre> <ol> <li>Then, configure profile for <code>monitoringEnv</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile monitoring-account\n</code></pre> <ol> <li>Login to required SSO profile using <code>aws sso login --profile &lt;profile name&gt;</code>. Let's now log in to <code>pipelineEnv</code> account. When SSO login expires, you can use this command to re-login.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\naws sso login --profile $AWS_PROFILE\n</code></pre> <ol> <li>Export required environment variables for further use. If not available already, you will be prompted for name of Amazon Grafana workspace in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. And, then its endpoint URL, ID, Role ARN will be captured as environment variables.</li> </ol> <pre><code>source `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/source-envs.sh\n</code></pre> <ol> <li>Create SSM SecureString Parameter <code>/cdk-accelerator/cdk-context</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account. This parameter contains account ID and region of all four AWS accounts used in this Observability Accelerator pattern.</li> </ol> <pre><code>aws ssm put-parameter --profile pipeline-account --region ${COA_PIPELINE_REGION} \\\n    --type \"SecureString\" \\\n    --overwrite \\\n    --name \"/cdk-accelerator/cdk-context\" \\\n    --description \"AWS account details of different environments used by Multi-Account Multi-Region Mixed Observability (M3) Accelerator pattern\" \\\n    --value '{\n        \"context\": {\n            \"pipelineEnv\": {\n                \"account\": \"'$COA_PIPELINE_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PIPELINE_REGION'\"\n\n            },\n            \"prodEnv1\": {\n                \"account\": \"'$COA_PROD1_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PROD1_REGION'\"\n            },\n            \"prodEnv2\": {\n                \"account\": \"'$COA_PROD2_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PROD2_REGION'\"\n            },\n            \"monitoringEnv\": {\n                \"account\": \"'$COA_MON_ACCOUNT_ID'\",\n                \"region\": \"'$COA_MON_REGION'\"\n            }\n        }\n    }'\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#amazon-grafana-configuration","title":"Amazon Grafana Configuration","text":"<ol> <li> <p>Run <code>helpers/multi-acc-new-eks-mixed-observability-pattern/amg-preconfig.sh</code> script to</p> </li> <li> <p>create SSM SecureString parameter <code>/cdk-accelerator/amg-info</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account. This will be used by CDK for Grafana Operator resources configuration.</p> </li> <li>create Grafana workspace API key.</li> <li>create SSM SecureString parameter <code>/cdk-accelerator/grafana-api-key</code> in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. This will be used by External Secrets Operator.</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/amg-preconfig.sh\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#github-sources-configuration","title":"GitHub Sources Configuration","text":"<ol> <li> <p>Following GitHub sources used in this pattern:</p> </li> <li> <p>Apps Git Repo - This repository serves as the source for deploying and managing applications in <code>prodEnv1</code> and <code>prodEnv2</code> using GitOps by Argo CD. Here, it is configured to sample-apps from aws-observability-accelerator.</p> </li> <li>Source for CodePipeline - This repository serves as the CodePipeline source stage for retrieving and providing source code to downstream pipeline stages, facilitating automated CI/CD processes. Whenever a change is detected in the source code, the pipeline is initiated automatically. This is achieved using GitHub webhooks. We are using CodePipeline to deploy multi-account multi-region clusters.</li> <li>Source for <code>monitoringEnv</code> Argo CD - This repository serves as the source for deploying and managing applications in the <code>monitoringEnv</code> environment using GitOps by Argo CD. Here, it is configured to grafana-operator-app from aws-observability-accelerator, using which Grafana Datasoures and Dashboards are deployed.</li> </ol> <p>NOTE: Argo CD source repositories used here for <code>prodEnv1</code>, <code>prodEnv2</code> and <code>monitoringEnv</code> are public. If you need to use private repositories, create secret called <code>github-ssh-key</code> in respective accounts and region. This secret should contain your GitHub SSH private key as a JSON structure with fields <code>sshPrivateKey</code> and <code>url</code> in AWS Secrets Manager. Argo CD add-on will use this secret for authentication with private GitHub repositories. For more details on setting up SSH credentials, please refer to Argo CD Secrets Support.</p> <ol> <li>Fork <code>cdk-aws-observability-accelerator</code> repository to your GitHub account.</li> <li> <p>Create GitHub Personal Access Token (PAT) for your CodePipeline GitHub source. For more information on how to set it up, please refer here. The GitHub Personal Access Token should have these scopes:</p> </li> <li> <p>repo - to read the repository</p> </li> <li> <p>admin:repo_hook - to use webhooks</p> </li> <li> <p>Run <code>helpers/multi-acc-new-eks-mixed-observability-pattern/gitsource-preconfig.sh</code> script to</p> </li> <li> <p>create SSM SecureString Parameter <code>/cdk-accelerator/pipeline-git-info</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account which contains details of CodePipeline source. This parameter contains GitHub owner name where you forked <code>cdk-aws-observability-accelerator</code>, repository name (<code>cdk-aws-observability-accelerator</code>) and branch (<code>main</code>).</p> </li> <li>create AWS Secret Manager secret <code>github-token</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account to hold GitHub Personal Access Token (PAT).</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/gitsource-preconfig.sh\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#deployment","title":"Deployment","text":"<ol> <li>Fork <code>cdk-aws-observability-accelerator</code> repository to your CodePioeline source GitHub organization/user.</li> <li>Install the AWS CDK Toolkit globally on host machine.</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Install project dependencies.</li> </ol> <pre><code>cd `git rev-parse --show-toplevel`\nnpm i\nmake build\n</code></pre> <ol> <li>Bootstrap all 4 AWS accounts using step mentioned for different environment for deploying CDK applications in Deploying Pipelines. If you have bootstrapped earlier, please remove them before proceeding with this step. Remember to set <code>pipelineEnv</code> account number in <code>--trust</code> flag. You can also refer to commands mentioned below:</li> </ol> <pre><code># bootstrap pipelineEnv account WITHOUT explicit trust\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile pipeline-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    aws://${COA_PIPELINE_ACCOUNT_ID}/${COA_PIPELINE_REGION}\n\n# bootstrap prodEnv1 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile prod1-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_PROD1_ACCOUNT_ID}/${COA_PROD1_REGION}\n\n# bootstrap prodEnv2 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile prod2-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_PROD2_ACCOUNT_ID}/${COA_PROD2_REGION}\n\n# bootstrap monitoringEnv account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile monitoring-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_MON_ACCOUNT_ID}/${COA_MON_REGION}\n</code></pre> <ol> <li>Once all pre-requisites are set, you are ready to deploy the pipeline. Run the following command from the root of cloned repository to deploy the pipeline stack in <code>pipelineEnv</code> account. This step may require approximately 20 minutes to finish.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\nexport AWS_REGION=${COA_PIPELINE_REGION}\ncd `git rev-parse --show-toplevel`\n\nmake pattern multi-acc-new-eks-mixed-observability deploy multi-account-COA-pipeline\n</code></pre> <ol> <li> <p>Check status of pipeline that deploys multiple Amazon EKS clusters through CloudFromation stacks in respective accounts. This deployment also creates</p> </li> <li> <p><code>ampPrometheusDataSourceRole</code> with permissions to retrieve metrics from AMP in <code>ProdEnv1</code> account,</p> </li> <li><code>cloudwatchDataSourceRole</code> with permissions to retrieve metrics from CloudWatch in <code>ProdEnv2</code> account and</li> <li>Updates Amazon Grafana workspace IAM role in <code>monitoringEnv</code> account to assume roles in <code>ProdEnv1</code> and <code>ProdEnv2</code> accounts for retrieving and visualizing metrics in Grafana</li> </ol> <p>This step may require approximately 50 minutes to finish. You may login to <code>pipelineEnv</code> account and navigate to AWS CodePipeline console at <code>pipelineEnv</code> region to check the status.</p> <pre><code># script to check status of codepipeline\ndots=\"\"; while true; do status=$(aws codepipeline --profile pipeline-account list-pipeline-executions --pipeline-name multi-account-COA-pipeline --query 'pipelineExecutionSummaries[0].status' --output text); [ $status == \"Succeeded\" ] &amp;&amp; echo -e \"Pipeline execution SUCCEEDED.\" &amp;&amp; break || [ \"$status\" == \"Failed\" ] &amp;&amp; echo -e \"Pipeline execution FAILED.\" &amp;&amp; break ||  printf \"\\r\" &amp;&amp; echo -n \"Pipeline execution status: $status$dots\" &amp;&amp; dots+=\".\" &amp;&amp; sleep 10; done\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#post-deployment","title":"Post Deployment","text":"<ol> <li> <p>Once all steps of <code>multi-acc-stages</code> in <code>multi-account-COA-pipeline</code> are complete, run script to</p> </li> <li> <p>create entries in kubeconfig with contexts of newly created EKS clusters.</p> </li> <li>export cluster specific and kubecontext environment variables (like: <code>COA_PROD1_CLUSTER_NAME</code> and <code>COA_PROD1_KUBE_CONTEXT</code>).</li> <li>get Amazon Prometheus Endpoint URL from <code>ProdEnv1</code> account and export to environment variable <code>COA_AMP_ENDPOINT_URL</code>.</li> </ol> <pre><code>source `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/post-deployment-source-envs.sh\n</code></pre> <ol> <li>Then, update parameter <code>AMP_ENDPOINT_URL</code> of Argo CD bootstrap app in <code>monitoringEnv</code> with Amazon Prometheus endpoint URL from <code>ProdEnv1</code> account (<code>COA_AMP_ENDPOINT_URL</code>) and sync Argo CD apps.</li> </ol> <pre><code>if [[ `lsof -i:8080 | wc -l` -eq 0 ]]\nthen\n    export ARGO_SERVER=$(kubectl --context ${COA_MON_KUBE_CONTEXT} -n argocd get svc -l app.kubernetes.io/name=argocd-server -o name)\n    export ARGO_PASSWORD=$(kubectl --context ${COA_MON_KUBE_CONTEXT} -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\n    echo \"ARGO PASSWORD:: \"$ARGO_PASSWORD\n    kubectl --context ${COA_MON_KUBE_CONTEXT} port-forward $ARGO_SERVER -n argocd 8080:443 &gt; /dev/null 2&gt;&amp;1 &amp;\n    argocdPid=$!\n    echo pid: $argocdPid\n    sleep 5s\n\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} login localhost:8080 --insecure --username admin --password $ARGO_PASSWORD\n\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} app set argocd/bootstrap-apps --helm-set AMP_ENDPOINT_URL=$COA_AMP_ENDPOINT_URL\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} app sync argocd/bootstrap-apps\n\n    echo -e '\\033[0;33m' \"\\nConfirm update here.. You should see AMP endpoint URL and no error message.\" '\\033[0m'\n    kubectl --context ${COA_MON_KUBE_CONTEXT} get -n grafana-operator grafanadatasources grafanadatasource-amp -o jsonpath='{.spec.datasource.url}{\"\\n\"}{.status}{\"\\n\"}'\n\n    kill -9 $argocdPid\nelse\n    echo \"Port 8080 is already in use by PID `lsof -i:8080 -t`. Please terminate it and rerun this step.\"\nfi\n</code></pre> <p>NOTE: You can access Argo CD Admin UI using port-forwading. Here are commands to access <code>prodEnv1</code> Argo CD:</p> <pre><code>export PROD1_ARGO_SERVER=$(kubectl --context ${COA_PROD1_KUBE_CONTEXT} -n argocd get svc -l app.kubernetes.io/name=argocd-server -o name)\nexport PROD1_ARGO_PASSWORD=$(kubectl --context ${COA_PROD1_KUBE_CONTEXT} -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\necho \"PROD1 ARGO PASSWORD:: \"$PROD1_ARGO_PASSWORD\nnohup kubectl --context ${COA_PROD1_KUBE_CONTEXT} port-forward $PROD1_ARGO_SERVER -n argocd 8081:443 &gt; /dev/null 2&gt;&amp;1 &amp;\nsleep 5\ncurl localhost:8081\n</code></pre> <ol> <li>Datasource <code>grafana-operator-amp-datasource</code> created by Grafana Operator needs to reflect AMP Endpoint URL. There is a limitation with Grafana Operator (or Grafana) which doesn't sync updated <code>grafana-datasources</code> to Grafana. To overcome this issue, we will simply delete Datasource and Grafana Operator syncs up with the latest configuration in 5 minutes. This is achieved using Grafana API and key stored in SecureString parameter <code>/cdk-accelerator/grafana-api-key</code> in <code>monitoringEnv</code> account.</li> </ol> <pre><code>export COA_AMG_WORKSPACE_URL=$(aws ssm get-parameter --profile pipeline-account --region ${COA_PIPELINE_REGION} \\\n    --name \"/cdk-accelerator/amg-info\" \\\n    --with-decryption \\\n    --query Parameter.Value --output text | jq -r \".[] | .workspaceURL\")\n\nexport COA_AMG_API_KEY=$(aws ssm get-parameter --profile monitoring-account --region ${COA_MON_REGION} \\\n    --name \"/cdk-accelerator/grafana-api-key\" \\\n    --with-decryption \\\n    --query Parameter.Value --output text)\n\nexport COA_AMP_DS_ID=$(curl -s -H \"Authorization: Bearer ${COA_AMG_API_KEY}\" ${COA_AMG_WORKSPACE_URL}/api/datasources \\\n    | jq -r \".[] |  select(.name==\\\"grafana-operator-amp-datasource\\\") | .id\")\n\necho \"Datasource Name:: grafana-operator-amp-datasource\"\necho \"Datasource ID:: \"$COA_AMP_DS_ID\n\ncurl -X DELETE -H \"Authorization: Bearer ${COA_AMG_API_KEY}\" ${COA_AMG_WORKSPACE_URL}/api/datasources/${COA_AMP_DS_ID}\n</code></pre> <ol> <li>Then, deploy ContainerInsights in <code>ProdEnv2</code> account.</li> </ol> <pre><code>prod2NGRole=$(aws cloudformation describe-stack-resources --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --stack-name \"coa-eks-prod2-${COA_PROD2_REGION}-coa-eks-prod2-${COA_PROD2_REGION}-blueprint\" \\\n    --query \"StackResources[?ResourceType=='AWS::IAM::Role' &amp;&amp; contains(LogicalResourceId,'NodeGroupRole')].PhysicalResourceId\" \\\n    --output text)\n\naws iam attach-role-policy --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --role-name ${prod2NGRole} \\\n    --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\n\naws iam list-attached-role-policies --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --role-name $prod2NGRole | grep CloudWatchAgentServerPolicy || echo 'Policy not found'\n\nFluentBitHttpPort='2020'\nFluentBitReadFromHead='Off'\n[[ ${FluentBitReadFromHead} = 'On' ]] &amp;&amp; FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'\n[[ -z ${FluentBitHttpPort} ]] &amp;&amp; FluentBitHttpServer='Off' || FluentBitHttpServer='On'\ncurl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml | sed 's/{{cluster_name}}/'${COA_PROD2_CLUSTER_NAME}'/;s/{{region_name}}/'${COA_PROD2_REGION}'/;s/{{http_server_toggle}}/\"'${FluentBitHttpServer}'\"/;s/{{http_server_port}}/\"'${FluentBitHttpPort}'\"/;s/{{read_from_head}}/\"'${FluentBitReadFromHead}'\"/;s/{{read_from_tail}}/\"'${FluentBitReadFromTail}'\"/' | kubectl --context ${COA_PROD2_KUBE_CONTEXT} apply -f -\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#validating-grafana-dashboards","title":"Validating Grafana Dashboards","text":"<ol> <li>Run the below command in <code>ProdEnv1</code> cluster to generate test traffic to sample application and let us visualize traces to X-Ray and Amazon Managed Grafana Console out the sample <code>ho11y</code> app :</li> </ol> <pre><code>frontend_pod=`kubectl --context ${COA_PROD1_KUBE_CONTEXT} get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec --context ${COA_PROD1_KUBE_CONTEXT} -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <ol> <li>Let it run for a few minutes and look in Amazon Grafana Dashboards &gt; Observability Accelerator Dashboards &gt; Kubernetes / Compute Resources / Namespace (Workloads)</li> </ol> <p>Please also have a look at other Dashboards created using Grafana Operator under folder Observability Accelerator Dashboards.</p> <ol> <li>Run the below command in <code>ProdEnv2</code> cluster to generate test traffic to sample application.</li> </ol> <pre><code>frontend_pod=`kubectl --context ${COA_PROD2_KUBE_CONTEXT} get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec --context ${COA_PROD2_KUBE_CONTEXT} -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <ol> <li>Let it run for a few minutes and look in Amazon Grafana Administration &gt; Datasources &gt; grafana-operator-cloudwatch-datasource &gt; Explore. Set values as highlighted in the snapshot and 'Run query'.</li> </ol> <p></p> <ol> <li>Then, let us look at X-Ray traces in Amazon Grafana Administration &gt; Datasources &gt; grafana-operator-xray-datasource &gt; Explore. Set Query Type = Service Map and 'Run query'.</li> </ol> <p></p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#clean-up","title":"Clean up","text":"<ol> <li>Run this command to destroy this pattern. This will delete pipeline.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\naws sso login --profile $AWS_PROFILE\ncd `git rev-parse --show-toplevel`\n\nsource `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/source-envs.sh\nmake pattern multi-acc-new-eks-mixed-observability destroy multi-account-COA-pipeline\n</code></pre> <ol> <li>Next, run this script to clean up resources created in respective accounts. This script deletes Argo CD apps, unsets kubeconfig entries, initiates deletion of CloudFormation stacks, secrets, SSM parameters and Amazon Grafana Workspace API key from respective accounts.</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/clean-up.sh\n</code></pre> <ol> <li>In certain scenarios, CloudFormation stack deletion might encounter issues when attempting to delete a nodegroup IAM role. In such situations, it's recommended to first delete the relevant IAM role and then proceed with deleting the CloudFormation stack.</li> <li>Delete Dashboards and Data sources in Amazon Grafana.</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/","title":"Single Cluster Open Source Observability - API Server Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with API Server monitoring.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_WORKLOADS_API_BASIC_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_WORKLOADS_API_ADVANCED_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_WORKLOADS_API_TROUBLESHOOTING_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"apiserver.pattern.enabled\": true,\n  }\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>Kubernetes/Kube-apiserver (basic), Kubernetes/Kube-apiserver (advanced), Kubernetes/Kube-apiserver (troubleshooting)</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (basic)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (advanced)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (troubleshooting)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/","title":"Single Cluster AWS Native Observability - Fargate","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Fargate Cluster Native Observability pattern using AWS native tools such as CloudWatch Logs and Container Insights.</p> <p></p> <p>This example makes use of CloudWatch Container Insights as a vizualization and metric-aggregation layer. Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards.</p> <p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. With ADOT, users can instrument their applications just once to send correlated metrics and traces to multiple monitoring solutions. With ADOT support for CloudWatch Container Insights, customers can collect system metrics such as CPU, memory, disk, and network usage from Amazon EKS clusters running on Amazon Elastic Cloud Compute (Amazon EC2), providing the same experience as Amazon CloudWatch agent. In EKS Fargate networking architecture, a pod is not allowed to directly reach the kubelet on that worker node. Hence, the ADOT Collector calls the Kubernetes API Server to proxy the connection to the kubelet on a worker node, and collect kubelet\u2019s cAdvisor metrics for workloads on that node. </p> <p>By combining Container Insights and CloudWatch logs, we are able to provide a foundation for EKS (Amazon Elastic Kubernetes Service) Observability. Monitoring EKS for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS Fargate cluster.</li> <li>Logs with CloudWatch Logs</li> <li>Enables CloudWatch Container Insights.</li> <li>Installs Prometheus Node Exporter and Metrics Server for infrastructure metrics.</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-awsnative-fargate-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-awsnative-fargate-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-awsnative-singleneweksawsnativeobs-xxxxxxxx\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>Output:\nNAME                                   STATUS   ROLES    AGE   VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nfargate-ip-10-0-102-84.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.102.84    &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-124-175.ec2.internal   Ready    &lt;none&gt;   12m   v1.27.1-eks-2f008fe   10.0.124.175   &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-126-244.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.126.244   &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-132-165.ec2.internal   Ready    &lt;none&gt;   12m   v1.27.1-eks-2f008fe   10.0.132.165   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-159-96.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.159.96    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-170-28.ec2.internal    Ready    &lt;none&gt;   14m   v1.27.1-eks-2f008fe   10.0.170.28    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-173-57.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.173.57    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-175-87.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.175.87    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-187-27.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.187.27    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-188-225.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.188.225   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-189-234.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.189.234   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-96-29.ec2.internal     Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.96.29     &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\n</code></pre></p> <p><pre><code>kubectl get pods -o wide -A\n</code></pre> <pre><code>NAMESPACE                       NAME                                                   READY   STATUS    RESTARTS       AGE   IP             NODE                                   NOMINATED NODE   READINESS GATES\ncert-manager                    cert-manager-875c7579b-5kzg5                           1/1     Running   0              17m   10.0.188.225   fargate-ip-10-0-188-225.ec2.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-cainjector-7bb6786867-xrtbx               1/1     Running   0              17m   10.0.102.84    fargate-ip-10-0-102-84.ec2.internal    &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-webhook-79d574fbd5-9b7mx                  1/1     Running   0              17m   10.0.187.27    fargate-ip-10-0-187-27.ec2.internal    &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-cloudwatch-collector-65bb5d7cb6-x8gdl   1/1     Running   1 (114s ago)   14m   10.0.132.165   fargate-ip-10-0-132-165.ec2.internal   &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-xray-collector-796b57b657-tnx86         1/1     Running   0              14m   10.0.124.175   fargate-ip-10-0-124-175.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-8dcffbf6c-6qgfn           1/1     Running   0              17m   10.0.96.29     fargate-ip-10-0-96-29.ec2.internal     &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-8dcffbf6c-dgqn6           1/1     Running   0              17m   10.0.189.234   fargate-ip-10-0-189-234.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-metrics-server-6765c9bc59-v98h5       1/1     Running   0              17m   10.0.175.87    fargate-ip-10-0-175-87.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-788dbcccd5-7lf2g                               1/1     Running   0              17m   10.0.173.57    fargate-ip-10-0-173-57.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-788dbcccd5-wn8nc                               1/1     Running   0              17m   10.0.126.244   fargate-ip-10-0-126-244.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-state-metrics-7f4b8b9f5-g994r                     1/1     Running   0              17m   10.0.159.96    fargate-ip-10-0-159-96.ec2.internal    &lt;none&gt;           &lt;none&gt;\nopentelemetry-operator-system   opentelemetry-operator-5fbdd4f5f9-lm2nf                2/2     Running   0              16m   10.0.170.28    fargate-ip-10-0-170-28.ec2.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p><pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <pre><code>NAME                       STATUS   AGE\naws-for-fluent-bit              Active   17m\ncert-manager                    Active   17m\ndefault                         Active   27m\nkube-node-lease                 Active   27m\nkube-public                     Active   27m\nkube-system                     Active   27m\nopentelemetry-operator-system   Active   17m\n</code></pre></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#viewing-logs","title":"Viewing Logs","text":"<p>By default, we deploy a FluentBit daemon set in the cluster to collect worker logs for all namespaces. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#using-cloudwatch-logs-insights-to-query-logs","title":"Using CloudWatch Logs Insights to Query Logs","text":"<p>Navigate to CloudWatch, then go to \"Logs Insights\"</p> <p>In the dropdown, select any of the logs that begin with \"/aws/eks/single-new-eks-awsnative-fargate-observability-accelerator\" and run a query.</p> <p>Example with \"kubesystem\" log group:</p> <p></p> <p>Then you can view the results of your query:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#viewing-metrics","title":"Viewing Metrics","text":"<p>Metrics are collected by the cloudWatchAdotAddon as based on the metricsNameSelectors we defined (default <code>['apiserver_request_.*', 'container_memory_.*', 'container_threads', 'otelcol_process_.*']</code>). These metrics can be found in the Cloudwatch metrics dashboard. </p> <p>Navigate to Cloudwatch, then go to \"Metrics\"</p> <p>Select \"All Metrics\" from the dropdown and select any logs in the ContainerInsights namespace</p> <p>Example with \"EKS_Cluster\" metrics</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#monitoring-workloads-on-eks","title":"Monitoring workloads on EKS","text":"<p>Although the default metrics exposed by cloudWatchAdotAddon are useful for getting some standardized metrics from our application we often instrument our own application with OLTP to expose metrics. Fortunately, the otel-collector-cloudwatch-collector can be specified as the endpoint for collecting these metrics and getting metrics and logs to cloudwatch. </p> <p>We will be fetching metrics from <code>ho11y</code> a synthetic signal generator allowing you to test observability solutions for microservices. It emits logs, metrics, and traces in a configurable manner. </p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#deploying-workload","title":"Deploying Workload","text":"<pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ho11y\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/z0a4o2j5/ho11y:latest\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"frontend\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: HO11Y_INJECT_FAILURE\n          value: \"enabled\"\n        - name: DOWNSTREAM0\n          value: \"http://downstream0\"\n        - name: DOWNSTREAM1\n          value: \"http://downstream1\"\n        imagePullPolicy: Always\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: downstream0\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: downstream0\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: downstream0\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/mhausenblas/ho11y:stable\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"downstream0\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: DOWNSTREAM0\n          value: \"https://mhausenblas.info/\"\n        imagePullPolicy: Always\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: downstream1\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: downstream1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: downstream1\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/mhausenblas/ho11y:stable\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"downstream1\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: DOWNSTREAM0\n          value: \"https://o11y.news/2021-03-01/\"\n        - name: DOWNSTREAM1\n          value: \"DUMMY:187kB:42ms\"\n        - name: DOWNSTREAM2\n          value: \"DUMMY:13kB:2ms\"\n        imagePullPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: frontend\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: downstream0\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: downstream0\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: downstream1\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: downstream1\n---\nEOF\n</code></pre> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>kubectl get pods\nNAME                                                   READY   STATUS    RESTARTS   AGE\ndownstream0-6b665bbfd-6zdsb                            1/1     Running   0          61s\ndownstream1-749d75f6c-9t5dl                            1/1     Running   0          61s\nfrontend-557fd48b4f-gx8ff                              1/1     Running   0          61s\n</code></pre> <p>Once deployed you will be able to monitor the Ho11y metrics in cloudwatch as shown:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-awsnative-fargate-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/","title":"Single Cluster AWS Native Observability","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Native Observability pattern using AWS native tools such as CloudWatch Logs and Container Insights.</p> <p></p> <p>This example makes use of CloudWatch Container Insights as a vizualization and metric-aggregation layer. Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards.</p> <p>By combining Container Insights and CloudWatch logs, we are able to provide a foundation for EKS (Amazon Elastic Kubernetes Service) Observability. Monitoring EKS for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>AWS Distro For OpenTelemetry Operator and Collector</li> <li>Logs with AWS for FluentBit and CloudWatch Logs</li> <li>Enables CloudWatch Container Insights.</li> <li>Installs Prometheus Node Exporter for infrastructure metrics.</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-awsnative-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-awsnative-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-awsnative-singleneweksawsnativeobs-JN3QM2KMBNCO\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.25.9-eks-0a21954   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                       STATUS   AGE\namazon-metrics             Active   10m\naws-for-fluent-bit         Active   10m\ncert-manager               Active   10m\ndefault                    Active   16m\nkube-node-lease            Active   16m\nkube-public                Active   16m\nkube-system                Active   16m\nprometheus-node-exporter   Active   10m\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#visualization","title":"Visualization","text":"<p>Navigate to CloudWatch and go to \"Container Insights\".</p> <p>View the Container Map:</p> <p></p> <p>View the Resource List:</p> <p></p> <p>View the Performance Monitoring Dashboard:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-awsnative-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/","title":"Single Cluster Setup","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#objective","title":"Objective","text":"<p>This pattern deploys one production grade Amazon EKS cluster, without any Observability add-on.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-cluster deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-observabil-singleneweksobservabilit-5NW0A5AUXVS9\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                            STATUS   ROLES    AGE     VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-157-151.eu-central-1.compute.internal   Ready    &lt;none&gt;   9m19s   v1.25.9-eks-0a21954   10.0.157.151   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                       STATUS   AGE\ncert-manager               Active   7m8s\ndefault                    Active   13m\nexternal-secrets           Active   7m9s\nkube-node-lease            Active   13m\nkube-public                Active   13m\nkube-system                Active   13m\nprometheus-node-exporter   Active   7m9s\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre> <p>aws</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/","title":"Single New EKS Cluster Opensource Observability - Fargate","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Fargate Open Source Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), FluentBit (Logs), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#metrics-and-traces","title":"Metrics and Traces","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. With ADOT, users can instrument their applications just once to send correlated metrics and traces to multiple monitoring solutions. </p> <p>The ADOT Collector has the concept of a pipeline which comprises three key types of components, namely, receiver, processor, and exporter. A receiver is how data gets into the collector. It accepts data in a specified format, translates it into the internal format and passes it to processors and exporters defined in the pipeline. It can be pull or push based. A processor is an optional component that is used to perform tasks such as batching, filtering, and transformations on data between being received and being exported. An exporter is used to determine which destination to send the metrics, logs or traces.</p> <p>In the above architecture, the kubelet on a worker node in a Kubernetes cluster exposes resource metrics such as CPU, memory, disk, and network usage at the /metrics/cadvisor endpoint. However, in EKS Fargate networking architecture, a pod is not allowed to directly reach the kubelet on that worker node. Hence, the ADOT Collector calls the Kubernetes API Server to proxy the connection to the kubelet on a worker node, and collect kubelet's cAdvisor metrics for workloads on that node. These metrics are made available in Prometheus format. Therefore, the collector uses an instance of Prometheus Receiver as a drop-in replacement for a Prometheus server and scrapes these metrics from the Kubernetes API server endpoint. Using Kubernetes service discovery, the receiver can discover all the worker nodes in an EKS cluster. Hence, more than one instances of ADOT Collector will suffice to collect resource metrics from all the nodes in a cluster. Having a single instance of ADOT collector can be overwhelming during higher loads so always recommend to deploy more than one collector.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#logs","title":"Logs","text":"<p>With Amazon EKS on Fargate, you can deploy pods without allocating or managing your Kubernetes nodes. This removes the need to capture system-level logs for your Kubernetes nodes. To capture the logs from your Fargate pods, we use Fluent Bit to forward the logs directly to CloudWatch. This enables you to automatically route logs to CloudWatch without further configuration or a sidecar container for your Amazon EKS pods on Fargate. For more information about this, see Fargate logging in the Amazon EKS documentation and Fluent Bit for Amazon EKS on the AWS Blog. This solution captures the STDOUT and STDERR input/output (I/O) streams from your container and sends them to CloudWatch through Fluent Bit, based on the Fluent Bit configuration established for the Amazon EKS cluster on Fargate</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern till step 7. At step 8, execute the following</p> <pre><code>make build\nmake pattern single-new-eks-fargate-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-fargate-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-fargate-op-singleneweksfargateopens-xxxxxxxx\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>Output:\nNAME                                   STATUS   ROLES    AGE   VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nfargate-ip-10-0-100-154.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.100.154   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-102-67.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.102.67    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-121-124.ec2.internal   Ready    &lt;none&gt;   7d11h   v1.27.1-eks-2f008fe   10.0.121.124   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-135-174.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.135.174   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-135-90.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.135.90    &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-136-121.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.136.121   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-138-15.ec2.internal    Ready    &lt;none&gt;   2d8h    v1.27.6-eks-f8587cb   10.0.138.15    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-139-227.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.139.227   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-148-152.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.148.152   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-151-22.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.151.22    &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-158-82.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.158.82    &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-161-151.ec2.internal   Ready    &lt;none&gt;   2d8h    v1.27.6-eks-f8587cb   10.0.161.151   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-164-251.ec2.internal   Ready    &lt;none&gt;   87m     v1.27.6-eks-f8587cb   10.0.164.251   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-165-99.ec2.internal    Ready    &lt;none&gt;   2d9h    v1.27.1-eks-2f008fe   10.0.165.99    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-167-115.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.167.115   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-178-170.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.178.170   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-186-44.ec2.internal    Ready    &lt;none&gt;   87m     v1.27.6-eks-f8587cb   10.0.186.44    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\n</code></pre></p> <p><pre><code>kubectl get pods -o wide -A\n</code></pre> <pre><code>NAMESPACE                       NAME                                                              READY   STATUS    RESTARTS        AGE     IP             NODE                                   NOMINATED NODE   READINESS GATES\ncert-manager                    cert-manager-8694c7d4fd-pwmhh                                     1/1     Running   0               12d     10.0.135.90    fargate-ip-10-0-135-90.ec2.internal    &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-cainjector-744cb68868-m2j25                          1/1     Running   0               12d     10.0.136.121   fargate-ip-10-0-136-121.ec2.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-webhook-5f6fff764b-4nq5q                             1/1     Running   0               12d     10.0.151.22    fargate-ip-10-0-151-22.ec2.internal    &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-amp-collector-7cc9cfb77f-kjp5b                     1/1     Running   0               2d9h    10.0.165.99    fargate-ip-10-0-165-99.ec2.internal    &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-797c97cc56-qnqvb                1/1     Running   0               12d     10.0.189.201   fargate-ip-10-0-189-201.ec2.internal   &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-cert-controller-75ccc646775f6   1/1     Running   0               12d     10.0.100.154   fargate-ip-10-0-100-154.ec2.internal   &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-webhook-749d46f5df-slb88        1/1     Running   0               12d     10.0.189.119   fargate-ip-10-0-189-119.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     helm-controller-69ff5c96c7-xkbpc                                  1/1     Running   0               12d     10.0.190.34    fargate-ip-10-0-190-34.ec2.internal    &lt;none&gt;           &lt;none&gt;\nflux-system                     image-automation-controller-65887476b7-8tvl6                      1/1     Running   0               12d     10.0.167.115   fargate-ip-10-0-167-115.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     image-reflector-controller-57847dc9cf-6pbts                       1/1     Running   0               12d     10.0.178.170   fargate-ip-10-0-178-170.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     kustomize-controller-68c6c766-hrxh4                               1/1     Running   0               12d     10.0.102.67    fargate-ip-10-0-102-67.ec2.internal    &lt;none&gt;           &lt;none&gt;\nflux-system                     notification-controller-5dbc9fc9c4-b7gvt                          1/1     Running   0               12d     10.0.188.107   fargate-ip-10-0-188-107.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     source-controller-5b669588f-jtgc6                                 1/1     Running   0               12d     10.0.148.152   fargate-ip-10-0-148-152.ec2.internal   &lt;none&gt;           &lt;none&gt;\ngrafana-operator                grafana-operator-7d7ccc88f4-fw99n                                 1/1     Running   0               12d     10.0.99.84     fargate-ip-10-0-99-84.ec2.internal     &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-7c7f88558d-rlzh6                     1/1     Running   0               12d     10.0.158.82    fargate-ip-10-0-158-82.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-7c7f88558d-v797p                     1/1     Running   1 (5h40m ago)   12d     10.0.190.41    fargate-ip-10-0-190-41.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-metrics-server-6765c9bc59-85jqq                  1/1     Running   0               90m     10.0.186.44    fargate-ip-10-0-186-44.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-6549dc85b9-586mh                                          1/1     Running   0               12d     10.0.139.227   fargate-ip-10-0-139-227.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-6549dc85b9-n6xdr                                          1/1     Running   0               12d     10.0.135.174   fargate-ip-10-0-135-174.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-state-metrics-596b5dbf46-c9mdt                               1/1     Running   0               90m     10.0.164.251   fargate-ip-10-0-164-251.ec2.internal   &lt;none&gt;           &lt;none&gt;\nopentelemetry-operator-system   opentelemetry-operator-5ddbdcdc57-nh5dr                           2/2     Running   0               7d11h   10.0.121.124   fargate-ip-10-0-121-124.ec2.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p><pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <pre><code>NAME                       STATUS   AGE\naws-for-fluent-bit              Active   12d\ncert-manager                    Active   12d\ndefault                         Active   12d\nexternal-secrets                Active   12d\nflux-system                     Active   12d\ngrafana-operator                Active   12d\nkube-node-lease                 Active   12d\nkube-public                     Active   12d\nkube-system                     Active   12d\nopentelemetry-operator-system   Active   7d11h\n</code></pre></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#visualize-dashboards-in-amazon-managed-grafana","title":"Visualize Dashboards in Amazon Managed Grafana","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a number of dashboards under <code>Observability Accelerator Dashboards</code>. Open <code>Kubernetes / Compute Resources / Cluster</code> and <code>Kubernetes / Kubelet</code> Dashboards, you should see data as below</p> <ul> <li>Cluster Dashboards</li> </ul> <p></p> <p>You can also visualize cluster metrics for specific namespaces by clicking on a particular namespace. For instance, below is a snapshot of the <code>kube-system</code> namespace.</p> <p></p> <ul> <li>Kubelet Dashboard</li> </ul> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#deploy-sample-java-workload","title":"Deploy Sample Java Workload","text":"<p>We'll deploy a sample java workload in our newly created EKS cluster running on AWS Fargate. To do that, follow the instructions in New EKS Cluster Java Open Source Observability Accelerator pattern till step number 4.</p> <p>Since we're deploying the sample workload on AWS Fargate compute, we need to create a Fargate profile for running the java application.</p> <p>Execute the following command to create a Fargate profile</p> <pre><code>SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\nCLUSTER_NAME=single-new-eks-fargate-opensource-observability-accelerator\neksctl create fargateprofile --namespace $SAMPLE_TRAFFIC_NAMESPACE --cluster $CLUSTER_NAME --name sample-java-workload-profile\n</code></pre> <p>Check if the profile got created successfully by running</p> <pre><code>$ eksctl get fargateprofile --cluster $CLUSTER_NAME\nNAME                                                            SELECTOR_NAMESPACE              SELECTOR_LABELS POD_EXECUTION_ROLE_ARN                                                                          SUBNETS          TAGS     STATUS\nsample-java-workload-profile                                    javajmx-sample                  &lt;none&gt;          arn:aws:iam::200202725330:role/eksctl-single-new-eks-farga-FargatePodExecutionRole-pBT3sLM15PYx subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-153ba837dcf44bbe84881aa5336f0bf1 default                         &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-1UCAAI02CIG27 subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-153ba837dcf44bbe84881aa5336f0bf1 kube-system                     &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-1UCAAI02CIG27 subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 cert-manager                    &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 external-secrets                &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 flux-system                     &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 grafana-operator                &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 opentelemetry-operator-system   &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\n</code></pre> <p>Now run step 5 of the instructions at New EKS Cluster Java Open Source Observability Accelerator pattern.</p> <p>Check whether the Sample Java Workload got deployed successfully</p> <pre><code>kubectl get po -n $SAMPLE_TRAFFIC_NAMESPACE\n\nNAME                             READY   STATUS    RESTARTS   AGE\ntomcat-bad-traffic-generator     1/1     Running   0          2d9h\ntomcat-example-fcbb8856b-s4mq8   1/1     Running   0          2d9h\ntomcat-traffic-generator         1/1     Running   0          2d9h\n</code></pre> <p>You should now see a new dashboard named <code>Java/JMX</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-fargate-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/","title":"Single Cluster Open Source Observability - NVIDIA GPU","text":"<p>Graphics Processing Units (GPUs) play an integral part in the Machine Learning (ML) workflow, by providing the scalable performance needed for fast ML training and cost-effective ML inference. On top of that, they are used in flexible remote virtual workstations and powerful HPC computations.</p> <p>This pattern shows you how to monitor the performance of the GPUs units, used in an Amazon EKS cluster leveraging GPU-based instances.</p> <p>Amazon Managed Service for Prometheus and Amazon Managed Grafana are open source tools used in this pattern to collect and visualise metrics respectively.</p> <p>Amazon Managed Service for Prometheus is a Prometheus-compatible service that monitors and provides alerts on containerized applications and infrastructure at scale.</p> <p>Amazon Managed Grafana is a managed service for Grafana, a popular open-source analytics platform that enables you to query, visualize, and alert on your metrics, logs, and traces.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#objective","title":"Objective","text":"<p>This pattern deploys an Amazon EKS cluster with a node group that includes instance types featuring NVIDIA GPUs.</p> <p>The AMI type of the node group is <code>AL2_x86_64_GPU AMI</code>, which uses the Amazon EKS-optimized Linux AMI with GPU support. In addition to the standard Amazon EKS-optimized AMI configuration, the GPU AMI includes the NVIDIA drivers.</p> <p>The NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA datacenter GPUs in cluster environments. It includes health monitoring, diagnostics, system alerts and governance policies. GPU metrics are exposed to Amazon Managed Service for Prometheus by the DCGM Exporter, that uses the Go bindings to collect GPU telemetry data from DCGM and then exposes the metrics for Amazon Managed Service for Prometheus to pull from, using an http endpoint (<code>/metrics</code>).</p> <p>The pattern deploys the NVIDIA GPU Operator add-on. The GPU Operator uses the NVIDIA DCGM Exporter to expose GPU telemetry to Amazon Managed Service for Prometheus.</p> <p>Data is visualised in Amazon Managed Grafana by the NVIDIA DCGM Exporter Dashboard.</p> <p>The rest of the setup to collect and visualise metrics with Amazon Managed Service for Prometheus and Amazon Managed Grafana, is similar to that used in other open-source based patterns included in this repository.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Amazon Managed Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys.</li> </ol> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/gpu\"\n        }\n      ]\n    },\n    \"gpuNodeGroup\": {\n      \"instanceType\": \"g4dn.xlarge\",\n      \"desiredSize\": 2, \n      \"minSize\": 2, \n      \"maxSize\": 3,\n      \"ebsSize\": 50\n    },\n  }\n</code></pre> <p>Note: insure your selected instance type is available in your region. To check that, you can run the following command (amend <code>Values</code> below as you see fit):</p> <pre><code>aws ec2 describe-instance-type-offerings \\\n    --filters Name=instance-type,Values=\"g4*\" \\\n    --query \"InstanceTypeOfferings[].InstanceType\" \\\n    --region us-east-2\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-gpu-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksgpuopensourc...\n</code></pre> <p>Let\u2019s verify the resources created by steps above:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Output:</p> <p></p> <p>Next, let's verify that each node has allocatable GPUs:</p> <pre><code>kubectl get nodes  \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n</code></pre> <p>Output:</p> <p></p> <p>We can now deploy the <code>nvidia-smi</code> binary, which shows diagnostic information about all GPUs visible to the container:</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: nvidia-smi\n    image: \"nvidia/cuda:11.0.3-base-ubuntu20.04\"\n    args:\n    - \"nvidia-smi\"\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <p>Then request the logs from the Pod:</p> <pre><code>kubectl logs nvidia-smi\n</code></pre> <p>Output:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#grafana-nvidia-dcgm-exporter-dashboard","title":"Grafana NVIDIA DCGM Exporter Dashboard","text":"<p>Login to your Amazon Managed Grafana workspace and navigate to the Dashboards panel. You should see a dashboard named <code>NVIDIA DCGM Exporter Dashboard</code>.</p> <p>We will now generate some load, to see some metrics in the dashboard. Please run the following command from terminal:</p> <pre><code>cat &lt;&lt; EOF | kubectl create -f -\n apiVersion: v1\n kind: Pod\n metadata:\n   name: dcgmproftester\n spec:\n   restartPolicy: OnFailure\n   containers:\n   - name: dcgmproftester11\n     image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04\n     args: [\"--no-dcgm-validation\", \"-t 1004\", \"-d 120\"]\n     resources:\n       limits:\n          nvidia.com/gpu: 1\n     securityContext:\n       capabilities:\n          add: [\"SYS_ADMIN\"]\nEOF\n</code></pre> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <p>Expected output:</p> <p></p> <p>After a few minutes, looking into the <code>NVIDIA DCGM Exporter Dashboard</code>, you should see the gathered metrics, similar to: </p> <p></p> <p></p> <p></p> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-gpu-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/","title":"Single Cluster Open Source Observability - Graviton","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Open Source Observability on Graviton pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#graviton","title":"Graviton","text":"<p>AWS Graviton Processors are designed by AWS to deliver the best price to performance for your cloud workloads running in Amazon EC2.  These processors are ARM chips running on aarch64 architecture. These processors feature key capabilities, such as the AWS Nitro System, that allow you to securely run cloud native applications at scale.</p> <p>Visit our EKS Blueprints docs for a list of supported addons on Graviton.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster running on a Graviton3 Processor</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys.</li> </ol> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS Secrets Manager for GRAFANA API KEY: Update the Grafana API key secret in AWS Secrets using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws secretsmanager create-secret \\\n    --name grafana-api-key \\\n    --description \"API Key of your Grafana Instance\" \\\n    --secret-string \"${AMG_API_KEY}\" \\\n    --region $AWS_REGION \\\n    --query ARN \\\n    --output text\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. </p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <p>If you need Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>If you want to deploy API Server dashboards along with Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true,\n    \"apiserver.pattern.enabled\": true,\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-graviton-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-graviton-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-gravitonop-singleneweksgravitonopens-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.27.1-eks-2f008fe   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.aarch64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\ncert-manager                    Active   2d1h\ndefault                         Active   2d1h\nexternal-secrets                Active   2d1h\nflux-system                     Active   2d1h\ngrafana-operator                Active   2d1h\nkube-node-lease                 Active   2d1h\nkube-public                     Active   2d1h\nkube-system                     Active   2d1h\nopentelemetry-operator-system   Active   2d1h\nprometheus-node-exporter        Active   2d1h\n</code></pre> <p>Next, lets verify all resources of <code>grafana-operator</code> namespace:</p> <pre><code>kubectl get all --namespace=grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/grafana-operator-866d4446bb-g5srl   1/1     Running   0          2d1h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service   ClusterIP   172.20.223.125   &lt;none&gt;        9090/TCP   2d1h\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator   1/1     1            1           2d1h\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-866d4446bb   1         1         1       2d1h\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the <code>Cluster</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Namespace (Workloads)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Node (Pods)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Workload</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Kubelet</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Nodes</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>From the cluster to view all dashboards as Kubernetes objects, run:</p> <pre><code>kubectl get grafanadashboards -A\n</code></pre> <pre><code>NAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to the \"Using CloudWatch Logs as a data source in Grafana\" section in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-graviton-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#1-grafana-dashboards-missing-or-grafana-api-key-expired","title":"1. Grafana dashboards missing or Grafana API key expired","text":"<p>In case you don't see the grafana dashboards in your Amazon Managed Grafana console, check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired. Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li>First, lets create a new Grafana API key.</li> </ul> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key-new\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ul> <li>Finally, update the Grafana API key secret in AWS Secrets Manager using the above new Grafana API key:</li> </ul> <pre><code>export API_KEY_SECRET_NAME=\"grafana-api-key\"\naws secretsmanager update-secret \\\n    --secret-id $API_KEY_SECRET_NAME \\\n    --secret-string \"${AMG_API_KEY}\" \\\n    --region $AWS_REGION\n</code></pre> <ul> <li>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/","title":"Single Cluster Open Source Observability - Java Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with Java based workloads.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Java workload.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#deploy-an-example-java-application","title":"Deploy an example Java application","text":"<p>In this section we will reuse an example from the AWS OpenTelemetry collector repository. For convenience, the steps can be found below.</p> <ol> <li> <p>Clone this repository and navigate to the <code>sample-apps/jmx/</code> directory.</p> </li> <li> <p>Authenticate to Amazon ECR (AWS_REGION was set during the deployment stage)</p> </li> </ol> <pre><code>export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n</code></pre> <ol> <li>Create an Amazon ECR repository</li> </ol> <pre><code>aws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\\n  --image-scanning-configuration scanOnPush=true \\\n  --region $AWS_REGION \n</code></pre> <ol> <li>Build Docker image and push to ECR.</li> </ol> <pre><code>docker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest .\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest \n</code></pre> <ol> <li>Install the sample application in the cluster</li> </ol> <pre><code>SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\ncurl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml | \nsed \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" |\nsed \"s/{{region}}/$AWS_REGION/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" | \nkubectl apply -f -\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE\n\nNAME                              READY   STATUS    RESTARTS   AGE\ntomcat-bad-traffic-generator      1/1     Running   0          90m\ntomcat-example-77b46cc546-z22jf   1/1     Running   0          25m\ntomcat-traffic-generator          1/1     Running   0          90m\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a new dashboard named <code>Java/JMX</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Java/JMX</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/","title":"Single Cluster AWS Mixed Observability","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Mixed Observability pattern using AWS native tools such as CloudWatch and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry(ADOT) and Prometheus Node Exporter.</p> <p></p> <p>This example makes use of CloudWatch as a metric and log aggregation layer while X-Ray is used as a trace-aggregation layer. In order to collect the metrics and traces we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p> <p>In this architecture AWS X-Ray provides a complete view of requests as they travel through your application and filters visual data across payloads, functions, traces, services, and APIs. X-Ray also allows you to perform analytics to gain powerful insights about your distributed trace data.</p> <p>Utilizing CloudWatch and X-Ray as an aggregation layer allows for a fully-managed scalable telemetry backend. In this example we get those benefits while still having the flexibility and rapid development of the Open Source collection tools.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>AWS Distro For OpenTelemetry Operator and Collector configured to collect metrics and traces.</li> <li>Logs with AWS for FluentBit and CloudWatch Logs</li> <li>Aggregate Metrics in CloudWatch</li> <li>Aggregate Traces in X-Ray</li> </ul> <p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-mixed-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-mixed-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksopensourceob-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-144-134.us-west-1.compute.internal   Ready    &lt;none&gt;   143m   v1.25.9-eks-0a21954   10.0.144.134   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\naws-for-fluent-bit              Active   142m\ncert-manager                    Active   142m\ndefault                         Active   148m\nexternal-secrets                Active   142m\nkube-node-lease                 Active   149m\nkube-public                     Active   149m\nkube-system                     Active   149m\nopentelemetry-operator-system   Active   142m\nprometheus-node-exporter        Active   142m\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#visualization","title":"Visualization","text":"<p>Navigate to CloudWatch and go to Metrics -&gt; All Metrics.</p> <p>Select the metrics in the ContainerInsights/Prometheus Namespace:</p> <p></p> <p>View the graph of the selected metrics:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-mixed-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/","title":"Single Cluster Open Source Observability - NGINX Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with Nginx based workloads.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NGINX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/nginx\"\n        }\n      ]\n    },\n    \"nginx.pattern.enabled\": true\n  }\n</code></pre> <p>!! warning This scenario might need larger worker node for the pod. </p> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Nginx workload.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#deploy-an-example-nginx-application","title":"Deploy an example Nginx application","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector.</p> <ol> <li> <p>Add NGINX ingress controller add-on into lib/single-new-eks-opensource-observability-pattern/index.ts in add-on array. <pre><code>        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.XrayAdotAddOn(),\n            new blueprints.addons.FluxCDAddOn({\"repositories\": [fluxRepository]}),\n            new GrafanaOperatorSecretAddon(),\n            new blueprints.addons.NginxAddOn({\n                name: \"ingress-nginx\",\n                chart: \"ingress-nginx\",\n                repository: \"https://kubernetes.github.io/ingress-nginx\",\n                version: \"4.7.2\",\n                namespace: \"nginx-ingress-sample\",\n                values: {\n                    controller: { \n                        metrics: {\n                            enabled: true,\n                            service: {\n                                annotations: {\n                                    \"prometheus.io/port\": \"10254\",\n                                    \"prometheus.io/scrape\": \"true\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }),\n        ];\n</code></pre></p> </li> <li> <p>Deploy pattern again  <pre><code>make pattern single-new-eks-opensource-observability deploy\n</code></pre></p> </li> <li> <p>Verify if the application is running <pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre></p> </li> <li> <p>Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller. <pre><code>EXTERNAL_IP=your-nginx-controller-external-ip\n</code></pre></p> </li> <li> <p>Start some sample NGINX traffic by entering the following command. <pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/master/k8s-deployment-manifest-templates/deployment-mode/service/cwagent-prometheus/sample_traffic/nginx-traffic/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f - \n</code></pre></p> </li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pod -n nginx-sample-traffic \n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a new dashboard named <code>NGINX</code>, under <code>Observability Accelerator Dashboards</code>.</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/","title":"Single Cluster Open Source Observability","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Open Source Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys.</li> </ol> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. </p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksopensourceob-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.25.9-eks-0a21954   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\ncert-manager                    Active   2d1h\ndefault                         Active   2d1h\nexternal-secrets                Active   2d1h\nflux-system                     Active   2d1h\ngrafana-operator                Active   2d1h\nkube-node-lease                 Active   2d1h\nkube-public                     Active   2d1h\nkube-system                     Active   2d1h\nopentelemetry-operator-system   Active   2d1h\nprometheus-node-exporter        Active   2d1h\n</code></pre> <p>Next, lets verify all resources of <code>grafana-operator</code> namespace:</p> <pre><code>kubectl get all --namespace=grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/grafana-operator-866d4446bb-g5srl   1/1     Running   0          2d1h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service   ClusterIP   172.20.223.125   &lt;none&gt;        9090/TCP   2d1h\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator   1/1     1            1           2d1h\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-866d4446bb   1         1         1       2d1h\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the <code>Cluster</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Namespace (Workloads)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Node (Pods)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Workload</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Kubelet</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Nodes</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>From the cluster to view all dashboards as Kubernetes objects, run:</p> <pre><code>kubectl get grafanadashboards -A\n</code></pre> <pre><code>NAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to the \"Using CloudWatch Logs as a data source in Grafana\" section in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#1-grafana-dashboards-missing-or-grafana-api-key-expired","title":"1. Grafana dashboards missing or Grafana API key expired","text":"<p>In case you don't see the grafana dashboards in your Amazon Managed Grafana console, check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired. Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li>First, lets create a new Grafana API key.</li> </ul> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key-new\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ul> <li>Finally, update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key:</li> </ul> <pre><code>export API_KEY_SECRET_NAME=\"grafana-api-key\"\naws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION \\\n    --overwrite\n</code></pre> <ul> <li>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"}]}